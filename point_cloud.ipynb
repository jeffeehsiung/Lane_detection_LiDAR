{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ego-Lane-fitting-Pointclouds\n",
    "+ Detects ego lane lines from pointcloud data, focusing on the top view to fit a 3-degree polynomial for the left and right lane lines. \n",
    "+ The algorithm's output must be the polynomial coefficients for each lane line, formatted similarly to the provided sample output.\n",
    "\n",
    "### Data Handling\n",
    "+ `Pointcloud Data`: \n",
    "    * work with binary files containing pointcloud data. \n",
    "    * Each point:  x, y, z, intensity, lidar beam values. \n",
    "    * The intensity and lidar beam values are crucial for distinguishing lane points.\n",
    "+ `Visualization`: The task provides a `data_visualize.py` script for visualizing pointcloud data, which can be a valuable tool for debugging and validating your algorithm's output.\n",
    "\n",
    "### Approach and Methodology\n",
    "Given a small dataset, traditional machine learning and geometric algorithms are more suitable for processing LiDAR pointcloud data for tasks such as ego lane detection. \n",
    "+ `Preprocessing`: \n",
    "    * Clean and filter the pointcloud data, focusing on points likely to represent lane markings based on intensity and possibly z values.\n",
    "+ `Lane Point Identification`: \n",
    "    * Use intensity and other heuristics to distinguish between lane and non-lane points. \n",
    "        * reflective scatter intensity and frequency of the lane is generally larger and higher than non-lane points (white/yellow for lane)\n",
    "        * threshold values obtain by exploiting clustering techniques to identify points belonging to lane markings accurately.\n",
    "+ `Lane Line Fitting`: Once you have identified lane points, \n",
    "    * fit a 3-degree polynomial to these points for both the left and right lanes. \n",
    "    * use curve fitting techniques or optimization algorithms to find the best fit.\n",
    "\n",
    "### Development\n",
    "+ `Libraries`: \n",
    "    * Open3D or Matplotlib for visualization.\n",
    "    * SciPy for fitting polynomials.\n",
    "+ `Algorithm Implementation`: Develop your algorithm step by step, starting with \n",
    "    * data loading\n",
    "    * data preprocessing\n",
    "        * `Voxel Grid Downsampling`: Reduces the density of the pointcloud by averaging points within each voxel, which can help speed up computations without losing significant detail.\n",
    "        * `Vertex normal estimation`: When the pointcloud will be used for surface reconstruction, rendering, or advanced analysis tasks\n",
    "            1. `Surface Reconstruction and Meshing`\n",
    "            For reconstructing surfaces from pointclouds, knowing the normals at each point is crucial. Normals indicate the orientation of the surface at each point, which helps algorithms like Poisson reconstruction or Greedy Triangulation to accurately generate meshes from the pointcloud data.\n",
    "            Normals are used to determine the \"inside\" and \"outside\" of a surface, which is essential for creating solid objects from pointclouds.\n",
    "            2. `Rendering and Visualization`\n",
    "            When visualizing pointclouds, especially in 3D rendering software, normals are used to apply lighting and shading effects correctly. Normals help in determining how light reflects off surfaces, which enhances the visual perception of depth and material properties in the rendered scene.\n",
    "            3. `Feature Extraction and Object Recognition`\n",
    "            In some advanced analysis tasks, such as identifying specific objects or features within a pointcloud, normals can provide additional geometric information that complements the raw position data. For example, the orientation of surface elements can help differentiate between vertical walls and horizontal roads.\n",
    "            4. `Improving Registration Accuracy`\n",
    "            For pointcloud registration tasks (aligning two pointclouds), normals can improve the accuracy of algorithms like Iterative Closest Point (ICP). By considering both the position and orientation of points, these algorithms can achieve more precise alignments, especially in scenes with complex geometries.\n",
    "            5. `Robustness to Noise and Downsampling`\n",
    "            Estimating normals after downsampling can help mitigate the effects of noise in the original pointcloud. By working with a reduced set of points, the normal estimation process can focus on the underlying surface's main features, potentially leading to more accurate and stable normals.\n",
    "    * feature extraction and filtering\n",
    "        * `DBSCAN`: After filtering, clustering can help group the remaining points into distinct lane markings based on proximity.\n",
    "        * `Intensity-based Filtering`: Since lane markings often have higher reflectivity than the road surface, intensity values can be used to filter points likely to represent lane markings.\n",
    "    * lane point identification\n",
    "        * `Ground Segmentation (optional)`: Separate the ground plane from other objects using geometric methods like RANSAC or a simple plane fitting algorithm, which is essential for focusing on lane markings.\n",
    "\n",
    "    * polynomial fitting\n",
    "        * `Polynomial Curve Fitting`: Once lane markings are identified, use methods like least squares to fit a polynomial curve to each detected lane marking. This step directly corresponds to the requirement of modeling lane lines with a 3-degree polynomial.\n",
    "        * `Iterative Closest Point (ICP) for Refinement`: In scenarios where prior lane models exist (e.g., from previous frames in a video), ICP can be used to refine the lane line fit by minimizing the distance between the new data points and the model.\n",
    "    * post-processing\n",
    "        * `Smoothing Filters`: Apply smoothing techniques to the polynomial coefficients to ensure lane lines are not overly sensitive to noise or outliers in the data. Techniques like moving averages or Savitzky-Golay filters can be effective.\n",
    "    * evlautation and validation\n",
    "        * `Manual Inspection`: Use visualization tools to manually inspect the fitted lane lines against the pointcloud data to ensure they accurately represent the ego lanes\n",
    "        * `Cross-validation`: If some labeled data is available, cross-validation techniques can be used to evaluate the robustness of your algorithm and optimize parameters.\n",
    "\n",
    "### Testing and Validation\n",
    "+ `Validation`: Use the provided sample_output to validate your algorithm's performance. Ensure your output format matches the expected format exactly.\n",
    "+ `Debugging and Optimization`: Use visualization tools to check the accuracy of lane detection and adjust your algorithm as necessary. Performance and accuracy are key.\n",
    "\n",
    "## Algorithms\n",
    "### Geometric and Heuristic Methods\n",
    "+ `RANSAC (Random Sample Consensus)`: Used for plane fitting and outlier removal. It's effective for identifying ground planes and other large flat surfaces by iteratively selecting a subset of points, fitting a model (e.g., a plane), and then measuring the model's validity against the entire dataset.\n",
    "+ `DBSCAN (Density-Based Spatial Clustering of Applications with Noise)`: A clustering algorithm that groups points closely packed together and marks points in low-density regions as outliers. It’s useful for segmenting objects from the background or separating different objects in a pointcloud.\n",
    "+ `Hough Transform`: Often used in image processing for line detection, it can also be applied to pointcloud data for detecting lane lines or other linear features by transforming points into a parameter space and detecting collinear points.\n",
    "+ `Euclidean Clustering`: A simple method to segment pointclouds into individual objects based on the Euclidean distance between points. It's effective for object detection when objects are well-separated in space.\n",
    "### Machine Learning and Deep Learning Methods (Not Suitable)\n",
    "+ `PointNet and Variants (PointNet++, DGCNN, etc.)`: Neural networks designed specifically for processing pointcloud data. They can classify, segment, and extract features from pointclouds directly, handling the data's unordered nature.\n",
    "+ `Convolutional Neural Networks (CNNs)`: While traditionally used for image data, CNNs can be applied to pointclouds that have been projected onto a 2D plane (e.g., bird’s-eye view) or converted into voxel grids or range images.\n",
    "+ `Graph Neural Networks (GNNs)`: Used for pointclouds modeled as graphs, where points are nodes and edges represent spatial or feature relationships. GNNs can capture complex patterns in the data for tasks like segmentation and object classification.\n",
    "+ `YOLO (You Only Look Once) for 3D`: Adaptations of popular real-time object detection systems to work with 3D data. For instance, projecting pointclouds into 2D spaces and then applying these algorithms to detect objects or features like lane lines.\n",
    "\n",
    "### Feature Extraction and Filtering Techniques\n",
    "+ `PCA (Principal Component Analysis)`: Used for dimensionality reduction and feature extraction. It can help identify the main directions of variance in the data, useful for tasks like ground plane removal or object orientation estimation.\n",
    "+ `Voxel Grid Filtering`: Reduces the resolution of pointcloud data by averaging points within voxel grids. It's a common preprocessing step to reduce computational load while preserving the overall structure of the scene.\n",
    "+ `Statistical Outlier Removal`:` Identifies and removes outliers based on the distribution of point-to-point distances, helping clean the data before further processing.\n",
    "\n",
    "### Curve Fitting and Optimization\n",
    "+ `Polynomial Curve Fitting`: Employed for lane detection, as mentioned in your task, where polynomial functions are fitted to data points representing lane markings.\n",
    "+ `Iterative Closest Point (ICP)`: An algorithm for aligning two pointclouds or a pointcloud and a model. It's used in tasks like SLAM (Simultaneous Localization and Mapping) for map building and localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data preprocessing\n",
    "import open3d as o3d\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from scipy.spatial import distance\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def load_pointclouds_with_attributes(folder_path):\n",
    "    \"\"\"\n",
    "    Loads a pointcloud from a binary file and parses it into a numpy array for further processing.\n",
    "    Given that open3D offers efficient pointcloud processing, we will use it to load the pointclouds.\n",
    "    Open3D to load and process pointcloud data, the primary focus is on spatial information (i.e., the x, y, z coordinates of each point). \n",
    "    By default, Open3D's PointCloud object does not directly handle non-spatial attributes.\n",
    "    Hence, relavant non-spatial attributes critical to ego lane detection can be managed alongside Open3D pointcloud objects.\n",
    "    Here, we returns a list of tuples containing Open3D pointcloud objects and corresponding attributes.\n",
    "    \n",
    "    PointCloud point and Attributes\n",
    "    ----------\n",
    "    1. scene: scene id - to be added to the point cloud\n",
    "    2. x: x coordinate of the point\n",
    "    3. y: y coordinate of the point\n",
    "    4. z: z coordinate of the point\n",
    "    5. intensity: intensity of the point\n",
    "    6. lidar_beam: lidar beam id\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    pointclouds_with_attributes = []\n",
    "    # get a list of all point cloud files in the directory\n",
    "    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.bin')]\n",
    "    # load each point cloud from the folder and assign a scene id to it using zip or enumerate\n",
    "    for scene_id, file_path in enumerate(file_paths):\n",
    "        # Load the binary pointcloud data as a NumPy array\n",
    "        points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 5)\n",
    "        \n",
    "        # Create an Open3D PointCloud object for spatial data (X, Y, Z)\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n",
    "\n",
    "        # add the scene id to the point cloud\n",
    "        attributes = np.c_[np.ones([points.shape[0], 1], dtype=np.int32) * scene_id, points[:, 3:]]\n",
    "        \n",
    "        # Add the tuple (Open3D pointcloud, attributes array) to the list\n",
    "        pointclouds_with_attributes.append((pcd, attributes))\n",
    "     \n",
    "    \n",
    "    return pointclouds_with_attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Voxel downsampling\n",
    "Voxel downsampling uses a regular voxel grid to create a uniformly downsampled point cloud from an input point cloud. It is often used as a pre-processing step for many point cloud processing tasks. The algorithm operates in two steps:\n",
    "1. Points are bucketed into voxels.\n",
    "2. Each occupied voxel generates exactly one point by averaging all points inside.\n",
    "<br>\n",
    "(source: open3D)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preprocess the point cloud data: downsample the point cloud\n",
    "def downsample_pointcloud(pcd, voxel_size=0.05):\n",
    "    \"\"\"\n",
    "    Downsamples the pointcloud using a voxel grid filter.\n",
    "    The algorithm operates in two steps:\n",
    "    1. Points are bucketed into voxels.\n",
    "    2. Each occupied voxel generates exactly one point by averaging all points inside.\n",
    "    \"\"\"\n",
    "    down_pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "    return down_pcd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex normal estimation\n",
    "Another basic operation for point cloud is point normal estimation. \n",
    "1. A normal at a point on a surface is a vector that is perpendicular (orthogonal) to the tangent plane at that point.\n",
    "2. Normals indicate the orientation of the surface at each point, which helps algorithms like Poisson reconstruction or Greedy Triangulation to accurately generate meshes from the pointcloud data.\n",
    "<br>\n",
    "(source: open3D)\n",
    "<br>\n",
    "\n",
    "#### Note\n",
    "In the context of ego lane detection, while normal estimation might not be directly necessary for identifying lane markings, understanding the procedure and its applications is valuable for broader pointcloud processing tasks and when considering the integration of pointcloud data into more complex scene analysis and reconstruction workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the downsampled point cloud data: compute the normals\n",
    "def compute_normals(downpcd, search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30)):\n",
    "    \"\"\"\n",
    "    Computes the normals of a pointcloud using the method proposed by [Rusu2009].\n",
    "    The algorithm operates in two steps:\n",
    "    1. A kD-tree is built for fast nearest neighbor search.\n",
    "    2. For each point, the normal vector is estimated by the covariance analysis of its k nearest neighbors in the estimate_normals method.\n",
    "    \n",
    "    The two key arguments radius = 0.1 and max_nn = 30 specifies search radius and maximum nearest neighbor. \n",
    "    It has 10cm of search radius, and only considers up to 30 neighbors to save computation time.\n",
    "    \n",
    "    Access estimated vertex normal:\n",
    "    - downpcd.normals - Returns a pointcloud with estimated vertex normals.\n",
    "    - normal = np.asarray(downpcd.normals)\n",
    "    \n",
    "    Returns a pointcloud with estimated vertex normals.\n",
    "    \"\"\"\n",
    "    downpcd.estimate_normals(search_param=search_param)    \n",
    "    return downpcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z Filtering\n",
    "Filter the scatters that is higher than ground and lanes by computing the mean and standard deviation of z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_filter(pointcloud):\n",
    "    \"\"\"\n",
    "    Filters the pointcloud based on the z-axis values.\n",
    "    The z-axis values of the pointcloud are used to filter the pointcloud.\n",
    "    \n",
    "    - pointcloud: Open3D pointcloud object\n",
    "    \n",
    "    Returns:\n",
    "    - selected_indices: Indices of the points that satisfy the z-axis filter\n",
    "    \n",
    "    \"\"\"\n",
    "    # Extract the z-axis values from the pointcloud\n",
    "    z = np.asarray(pointcloud.points)[:, 2]\n",
    "    \n",
    "    # Compute the mean and standard deviation of the z-axis values\n",
    "    mean = np.mean(z)\n",
    "    std = np.std(z)\n",
    "    \n",
    "    max_height = mean + 2 * std\n",
    "    \n",
    "    # Filter the pointcloud based on the z-axis values and return the indices of the selected points\n",
    "    selected_indices = np.where(z < max_height)[0]\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity-based Filtering Functions\n",
    "Selecting points from the pointcloud based on their intensity values, isolating potential lane markings, which typically have higher reflectivity\n",
    "1. apply `DBSCAN clustering` to group local point cloud clusters together to enable lanes intensity threshold learning\n",
    "2. Correlate the DBSCAN labels with the intensity values stored in the attributes array.\n",
    "3. For each cluster, based on the DBSCAN result, for each point cloud\n",
    "    * Intensity Threshold Learning: Determine the intensity threshold based on the intensity of each cluster\n",
    "4. Filter Clusters Based on Intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_dbscan_open3d(pcd, eps=0.08, min_samples=10, visualize=False):\n",
    "    \"\"\"\n",
    "    Applies DBSCAN clustering using Open3D to separate points based on spatial information.\n",
    "    \n",
    "    Parameters:\n",
    "    - pcd: An Open3D pointcloud object.\n",
    "    - eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    - min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    \n",
    "    Returns:\n",
    "    - pcd: The Open3D pointcloud object with colors assigned based on cluster labels.\n",
    "    - labels: Cluster labels for each point in the pointcloud.\n",
    "    \"\"\"\n",
    "    with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:\n",
    "        labels = np.array(pcd.cluster_dbscan(eps=eps, min_points=min_samples, print_progress=False))\n",
    "    \n",
    "    # Assign colors to points based on cluster labels for visualization\n",
    "    max_label = labels.max()\n",
    "    print(f\"Point cloud has {max_label + 1} clusters\")\n",
    "    colors = plt.get_cmap(\"tab20\")(labels / (max_label if max_label > 0 else 1))\n",
    "    colors[labels < 0] = 0  # Marking noise as black\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "    \n",
    "    # Visualize the clustered point cloud\n",
    "    if visualize:\n",
    "        o3d.visualization.draw_geometries([pcd], point_show_normal=False, window_name=\"Clustered Point Cloud\")\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering Functions\n",
    "* clustering\n",
    "* tuning eps and min_samples hyperparas: approached through grid search, random search, or more sophisticated optimization techniques, but it often involves a trade-off between computational cost and the quality of the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for clustering and evaluating the clustering results\n",
    "def evaluate_clustering(example_pcd, eps, min_samples):\n",
    "    labels = np.array(example_pcd.cluster_dbscan(eps=eps, min_points=min_samples, print_progress=False))\n",
    "    max_label = labels.max()\n",
    "    num_clusters = max_label + 1\n",
    "    noise_ratio = np.sum(labels == -1) / len(labels)\n",
    "    \n",
    "    return num_clusters, noise_ratio\n",
    "\n",
    "#  clustering scoring function\n",
    "def score_clustering(num_clusters, noise_ratio, optimal_clusters=32, noise_penalty=100):\n",
    "    \"\"\"\n",
    "    Scores the clustering outcome based on the number of clusters, noise ratio, and predefined targets.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_clusters: The number of clusters detected.\n",
    "    - noise_ratio: The ratio of points classified as noise.\n",
    "    - optimal_clusters: The target number of clusters for optimal scoring. (ground, lanes, crosswalks, trees, curb, vehicles, signs, barriers, trees, housing, beings.)\n",
    "    - noise_penalty: The weight of the noise ratio in the score calculation.\n",
    "    \n",
    "    Returns:\n",
    "    - score: A calculated score of the clustering outcome.\n",
    "    \"\"\"\n",
    "    # Reward configurations that are close to the optimal number of clusters\n",
    "    cluster_score = -abs(num_clusters - optimal_clusters)\n",
    "    \n",
    "    # Penalize configurations with a high noise ratio\n",
    "    noise_score = -noise_ratio * noise_penalty\n",
    "    \n",
    "    # Calculate total score\n",
    "    score = cluster_score + noise_score\n",
    "    return score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dbscan_and_update_attributes(pcd_with_attributes, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Performs DBSCAN clustering on the pointcloud and updates attributes with cluster labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - pcd_with_attributes: Tuple of (Open3D pointcloud, attributes array).\n",
    "    - eps: The DBSCAN eps parameter.\n",
    "    - min_samples: The DBSCAN min_samples parameter.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated tuple of (Open3D pointcloud, updated attributes array with DBSCAN labels).\n",
    "    \"\"\"\n",
    "    # Select a single pointcloud example for hyperparameter tuning\n",
    "    pcd, attributes = pcd_with_attributes\n",
    "    # Applying DBSCAN clustering with the best parameters: best_params['eps'] and best_params['min_samples']\n",
    "    best_labels = cluster_with_dbscan_open3d(pcd, eps, min_samples)\n",
    "    updated_attributes = np.hstack((attributes, best_labels.reshape(-1, 1)))  # Append labels as a new column\n",
    "    return (pcd, updated_attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_intensity_threshold_from_clusters(attributes, percentage=0.10):\n",
    "    \"\"\"\n",
    "    Learns an intensity threshold based on the analysis of clusters.\n",
    "    Parameters:\n",
    "    - attributes: A NumPy array containing DBSCAN labels and intensities.\n",
    "    Returns:\n",
    "    - An intensity threshold learned from the cluster analysis.\n",
    "    \"\"\"\n",
    "    # Assuming DBSCAN labels are the last column, and intensity is the second column\n",
    "    cluster_labels = attributes[:, -1].astype(int)\n",
    "    intensities = attributes[:, 1]\n",
    "    \n",
    "    # Initialize variables to store sum of intensities and count for each cluster\n",
    "    cluster_intensity_sum = {}\n",
    "    cluster_point_count = {}\n",
    "    \n",
    "    # Calculate sum of intensities and point count for each cluster\n",
    "    for label, intensity in zip(cluster_labels, intensities):\n",
    "        if label not in cluster_intensity_sum:\n",
    "            cluster_intensity_sum[label] = 0\n",
    "            cluster_point_count[label] = 0\n",
    "        cluster_intensity_sum[label] += intensity\n",
    "        cluster_point_count[label] += 1\n",
    "    \n",
    "    # Calculate average intensity and std for each cluster\n",
    "    mean_intensity = {label: cluster_intensity_sum[label] / cluster_point_count[label]\n",
    "                         for label in cluster_intensity_sum}\n",
    "    \n",
    "    # # Determine threshold as the average of top N highest average intensities\n",
    "    N = max(1, int(round(len(mean_intensity) * percentage)))  # Use top 75% of clusters and cast to int\n",
    "    top_average_intensities = sorted(mean_intensity.values(), reverse=True)[:N]\n",
    "    threshold = sum(top_average_intensities) / N\n",
    "    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_clusters_based_on_intensity(attributes, intensity_threshold):\n",
    "    \"\"\"\n",
    "    Filters clusters based on intensity and normals analysis.\n",
    "    Parameters:\n",
    "    - pcd: An Open3D pointcloud object with normals computed.\n",
    "    - attributes: A NumPy array with DBSCAN labels and intensities.\n",
    "    - intensity_threshold: Threshold for filtering based on intensity.\n",
    "    Returns:\n",
    "    - A list of indices for points considered as part of lanes.\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_labels = attributes[:, -1].astype(int)\n",
    "    intensities = attributes[:, 1]\n",
    "    \n",
    "    selected_indices = []\n",
    "    for i, (label, intensity) in enumerate(zip(cluster_labels, intensities)):\n",
    "        if label >= 0 and intensity > intensity_threshold:\n",
    "            selected_indices.append(i)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Plane Segmentation\n",
    "simplify further analysis like lane marking detection by reducing the data's complexity. One common approach for ground segmentation is to use the Random Sample Consensus (RANSAC) algorithm\n",
    "+  `distance_threshold`: \n",
    "    * defines the maximum distance a point can have to an estimated plane to be considered an inlier \n",
    "+ `ransac_n`:\n",
    "    * defines the number of points that are randomly sampled to estimate a plane\n",
    "+ `num_iterations`: \n",
    "    * defines how often a random plane is sampled and verified. \n",
    "The function then returns the plane as (a,b,c,d) such that for each point (x,y,z) on the plane we have ax +by + cz + d = 0. The function further returns a list of indices of the inlier points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_ground_plane(pcd, attributes, distance_threshold=0.01, ransac_n=3, num_iterations=1000):\n",
    "    \"\"\"\n",
    "    Segments the ground plane from an input point cloud using the RANSAC algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - pcd: Open3D point cloud object from which the ground plane is to be segmented.\n",
    "    - distance_threshold: Maximum distance a point can be from the plane model to be considered as an inlier.\n",
    "    - ransac_n: Number of points to sample for generating the plane model in each iteration.\n",
    "    - num_iterations: Number of iterations RANSAC will run to maximize inliers.\n",
    "\n",
    "    Returns:\n",
    "    - ground_pcd: The segmented ground plane as an Open3D point cloud.\n",
    "    - non_ground_pcd: All points that are not part of the ground plane as an Open3D point cloud.\n",
    "    \"\"\"\n",
    "    # Perform plane segmentation to separate ground\n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold=distance_threshold,\n",
    "                                             ransac_n=ransac_n,\n",
    "                                             num_iterations=num_iterations)\n",
    "    [a, b, c, d] = plane_model\n",
    "\n",
    "    # Extract inliers and outliers\n",
    "    inlier_cloud = pcd.select_by_index(inliers)\n",
    "    outlier_cloud = pcd.select_by_index(inliers, invert=True)\n",
    "    # create a list with zeros with length equal to the number of points in the point cloud\n",
    "    inniers_bool = np.zeros(len(pcd.points), dtype=bool)\n",
    "    inniers_bool[inliers] = True\n",
    "    \n",
    "    # Extract the inline points from the attributes and outliers from the attributes\n",
    "    inlier_attributes = attributes[inliers]\n",
    "    outlier_attributes = attributes[~inniers_bool]\n",
    "    \n",
    "    # Optional: Assign a unique color to ground and non-ground points for visualization\n",
    "    inlier_cloud.paint_uniform_color([0.0, 1.0, 0.0])  # Green for ground\n",
    "    outlier_cloud.paint_uniform_color([1.0, 0.0, 0.0])  # Red for non-ground\n",
    "\n",
    "    return inlier_cloud, outlier_cloud, inlier_attributes, outlier_attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Point Cloud Visualizatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lane_detection(original_pcd, filtered_pcd):\n",
    "    \"\"\"\n",
    "    Visualizes the original point cloud and the filtered point cloud representing detected lanes.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_pcd: The original Open3D point cloud object.\n",
    "    - filtered_pcd: The filtered Open3D point cloud object representing detected lanes.\n",
    "    \"\"\"\n",
    "    # Set the color of the original point cloud to gray\n",
    "    colors = np.asarray(original_pcd.colors)\n",
    "    gray_color = np.array([[0.5, 0.5, 0.5]])  # Gray color\n",
    "    if len(colors) == 0:  # If original point cloud has no colors\n",
    "        colors = np.tile(gray_color, (np.asarray(original_pcd.points).shape[0], 1))\n",
    "    else:\n",
    "        colors *= 0.5  # Darken the original colors to distinguish from filtered points\n",
    "    original_pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Set the color of the filtered point cloud to yellow for high visibility\n",
    "    lane_color = np.array([[1, 0.75, 0.8]])  # Pink color\n",
    "    num_filtered_points = np.asarray(filtered_pcd.points).shape[0]\n",
    "    filtered_colors = np.tile(lane_color, (num_filtered_points, 1))\n",
    "    filtered_pcd.colors = o3d.utility.Vector3dVector(filtered_colors)\n",
    "    \n",
    "    # Combine the original and filtered point clouds for visualization\n",
    "    combined_pcd = original_pcd + filtered_pcd\n",
    "    \n",
    "    # Visualize the combined point cloud\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=\"Before-Intensity-Filter Visualization\")\n",
    "    vis.add_geometry(combined_pcd)\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "  \n",
    "    # Visualize only the filtered point cloud for better visibility\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=\"After-Intensity-Filter Visualization\")\n",
    "    vis.add_geometry(combined_pcd)\n",
    "    vis.add_geometry(filtered_pcd)\n",
    "    vis.run()\n",
    "    vis.remove_geometry(combined_pcd, False)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    vis.close()\n",
    "    vis.destroy_window()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Detection\n",
    "- `Line Clustering`\n",
    "    * Polyline fitting requires lane clustering\n",
    "    * K-means clustering based on euclidean distance is applied to cluster lanes and crosswalks.\n",
    "    * the optimal k-mean cluster is determines based on the result of silhouette score nd visualized multiple cluster per k-mean value.\n",
    "    * the clustering visulization will utilize matlibplot scatter plot\n",
    "- `Lanes Detection`\n",
    "    * detect numner of lanes based on number of peak intensity in the range of y\n",
    "- `Line Fitting`\n",
    "    * For each cluster that potentially represents a lane, perform a polynomial fit. You can use np.polyfit with a degree of 2 or 3 (for a quadratic or cubic fit), which is common for lane fitting.\n",
    "- `Cost Function`\n",
    "    * To refine the fits or to choose between multiple candidate fits, define a cost function that penalizes fits which deviate significantly from what a typical lane would look like. This can include factors such as lane width, parallelism with other lanes, and orientation relative to the driving direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Clustering: K-Means Clustering\n",
    "* `Scaler Transform`: standardize the data by transforming based on standardscaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_transform(pointcloud):\n",
    "    '''\n",
    "    Transform the data by calculating the z-score of each value in the sample,\n",
    "    This is done by taking the feature x,y,z, subtract the mean of the feature and then divide by the standard deviation of the feature.\n",
    "    This process can be inflence by outliers, hence noise should be removed before scaling.\n",
    "    - pointcloud: Open3D pointcloud objects\n",
    "    \n",
    "    Returns:\n",
    "    - pointcloud_T: A new Open3D pointcloud objects with transformed coordinates that has the same shape as the original pointcloud.\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    # Extract the coordinates from the point cloud \n",
    "    coordinates = np.asarray(pointcloud.points)\n",
    "    # fit the scaler to the data, transform the data\n",
    "    xyz_T = scaler.fit_transform(coordinates)\n",
    "    # create a point cloud with the transformed coordinates\n",
    "    pointcloud_T = o3d.geometry.PointCloud()\n",
    "    pointcloud_T.points = o3d.utility.Vector3dVector(xyz_T)\n",
    "    return pointcloud_T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize K Means:  Elbow or Silhouette?\n",
    "* `Elbow Method`:\n",
    "    * Principle: The elbow method looks at the percentage of variance explained as a function of the number of clusters. You plot the number of clusters against the within-cluster sum of squares (WCSS), looking for an \"elbow\" where the rate of decrease sharply changes. This point is considered to be indicative of the appropriate number of clusters.\n",
    "    * Suitability for Elongated Features: The elbow method can sometimes struggle with identifying the correct number of clusters for data with elongated features because it relies on variance, which might not decrease significantly after a certain point if the clusters are elongated and dispersed.\n",
    "    * Use Case: It's more suitable when the clusters have a roughly spherical shape rather than elongated features. However, it can still provide a good initial estimate or insight into the clustering tendency of the dataset.\n",
    "* `Silhouette Method`:\n",
    "    * Principle: The silhouette method measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "    * Suitability for Elongated Features: The silhouette method can be more effective for evaluating the quality of clustering when dealing with elongated features. This is because it considers both cohesion (how close objects are to other objects in their cluster) and separation (how distinct a cluster is from other clusters), rather than just the variance within clusters.\n",
    "    * Use Case: It's especially useful when the data contains complex structures or when the clusters are not spherical. The silhouette score provides a more nuanced view of cluster quality and separation, making it a better choice for assessing the appropriateness of the number of clusters in cases with elongated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k_means(pointcloud_T, max_n_clusters = 12, visualize=False):\n",
    "    \"\"\"\n",
    "    Calculate silhouette score for a cluster of points to find the optimal number of clusters.\n",
    "    - pointcloud_T: scaler transformed Open3D pointcloud objects\n",
    "    - range_n_clusters: maximum integers representing the max range of clusters to consider.\n",
    "    - visualize: A boolean indicating whether to visualize the the cluster result or not.\n",
    "    \n",
    "    Returns:\n",
    "    - num_cluster: The optimal number of clusters\n",
    "    \"\"\"\n",
    "    # extract the x and y coordinates\n",
    "    xy_T = np.asarray(pointcloud_T.points)[:, :2]\n",
    "    \n",
    "    silhouette_score_buffer = []\n",
    "    # distortions = []\n",
    "    \n",
    "    if visualize:\n",
    "        df = pd.DataFrame(xy_T, columns=['x', 'y'])\n",
    "\n",
    "    min_n_cluster = 2\n",
    "    for k in range(min_n_cluster, max_n_clusters):\n",
    "        \n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        # take the nearest 2D points of max distance of 0.05 meters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10, n_init=10, max_iter=300)\n",
    "        cluster_labels = kmeans.fit_predict(xy_T)\n",
    "        \n",
    "        #  check also dbscan\n",
    "        db = DBSCAN(eps=(k*(0.01)), min_samples= 10).fit(xy_T)\n",
    "        labels = db.labels_\n",
    "        \n",
    "        # new columns for each kmeans cluster label in cluster_labels\n",
    "        df[f'KMeans_{k}'] = cluster_labels\n",
    "        df[f'DBSCAN_{k}'] = labels\n",
    "        \n",
    "        # calculate the silhouette score\n",
    "        silhouette_avg = silhouette_score(xy_T, cluster_labels)\n",
    "        \n",
    "        silhouette_score_buffer.append(silhouette_avg)\n",
    "        \n",
    "        # calculatee the elbow method to find the optimal number of clusters\n",
    "        # elbow method to find the optimal number of clusters\n",
    "        # distortions.append(kmeans.inertia_)\n",
    "        \n",
    "    if visualize: \n",
    "        # plot the clusters\n",
    "        fig, axs = plt.subplots(1, len(df.columns)-2, figsize=(40, 5))\n",
    "        for i, ax in enumerate(fig.axes, start=2):\n",
    "            ax.scatter(df['x'], df['y'], c=df.iloc[:, i], cmap='viridis')\n",
    "            # set titile same as the column name\n",
    "            ax.set_title(df.columns[i])\n",
    "        plt.show()\n",
    "        \n",
    "    # choose the number of clusters with the highest silhouette score\n",
    "    num_cluster = np.argmax(silhouette_score_buffer) + min_n_cluster\n",
    "    \n",
    "    # # Use the 'kneed' library to identify the elbow point automatically\n",
    "    # kn = KneeLocator(range(min_n_cluster, max_n_clusters), distortions, curve='convex', direction='decreasing')\n",
    "    # num_cluster = kn.knee\n",
    "    \n",
    "    return num_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  find the number of lanes based on the number of intensity peaks from range min y to max y\n",
    "def find_number_of_lanes(pointcloud, attributes, percentile=90, min_num_peaks=2):\n",
    "    \"\"\"\n",
    "    Find the number of lanes based on the number of intensity peaks.\n",
    "    - pointcloud: Open3D pointcloud object\n",
    "    - attributes: The attributes that contains intensities values of the point cloud at index 1\n",
    "    - min_num_peaks: The number of peaks to detect.\n",
    "    - percentage: The percentage of the intensity to consider as the threshold.\n",
    "    Returns:\n",
    "    - num_lanes: The number of lanes detected based on intensity peaks.\n",
    "    \"\"\"\n",
    "    # Extract the y-coordinates and intensities\n",
    "    y = np.asarray(pointcloud.points)[:, 1]\n",
    "    intensities = np.asarray(attributes[:, 1])\n",
    "    # convert intensities into dB scale and set the threshold intensity to the 90th percentile\n",
    "    # intensities = 10 * np.log10(intensities)\n",
    "    # threshold_intensity = np.percentile(intensities, percentile)\n",
    "    threshold_intensity = 1\n",
    "    \n",
    "    min_y, max_y = np.min(y), np.max(y)\n",
    "    \n",
    "    # pick the y bins that has the maximum intensity\n",
    "    lane_width = 3.75\n",
    "    max_num_lanes = 6\n",
    "    max_two_way_width = lane_width * max_num_lanes * 2\n",
    "    # define number of y bins based on the maximum two way width\n",
    "    y_bins = np.linspace(min_y, max_y, int(max_two_way_width))\n",
    "    intensity_histogram, _ = np.histogram(y, bins=y_bins, weights=intensities)\n",
    "    \n",
    "    # find the peaks in the intensity histogram that are larger than the threshold\n",
    "    peaks_bin, _ = find_peaks(intensity_histogram, height=0)\n",
    "    num_lanes = max(min_num_peaks, len(peaks_bin))\n",
    "    # convert back the y with peak intensity to the original scale\n",
    "    y_peak_coordinates = y_bins[peaks_bin]\n",
    "    print(f\"Number of lanes detected: {num_lanes} at y-coordinates: {y_peak_coordinates}\")\n",
    "    # plot the intensity histogram with the peaks\n",
    "    plt.plot(y_bins[1:], intensity_histogram)\n",
    "    plt.plot(y_bins[peaks_bin], intensity_histogram[peaks_bin], \"x\")\n",
    "    plt.xlabel('y-coordinate')\n",
    "    plt.ylabel('intensity')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return num_lanes, y_peak_coordinates, threshold_intensity\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slope(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate the slope between two points.\n",
    "    \"\"\"\n",
    "    if (point2[0] - point1[0]) == 0:  # Avoid division by zero\n",
    "        return np.inf\n",
    "    return (point2[1] - point1[1]) / (point2[0] - point1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segregate_points_based_on_lanes(filtered_points, filtered_attributes, y_peak_coordinates, threshold_intensity, num_bins=100):\n",
    "    \"\"\"\n",
    "    Improved segregation of points based on lanes by analyzing each X-coordinate bin for lane detection\n",
    "    based on Y-coordinates and intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - filtered_points: open3D pointcloud object\n",
    "    - filtered_attributes: attributes corresponding to filtered_points, including intensity.\n",
    "    - y_peak_coordinates: Y-coordinates representing the peak locations of lanes.\n",
    "    - threshold_intensity: Intensity threshold to consider a point as part of a lane.\n",
    "    - num_bins: Number of bins to divide the X-coordinate range into.\n",
    "\n",
    "    Returns:\n",
    "    - lane_groups: A dictionary where keys are tuples (bin_index, lane_index) and values are numpy arrays of points assigned to a lane.\n",
    "    \"\"\"\n",
    "    # Initialize the result dictionary\n",
    "    lane_groups = {}\n",
    "    # to np.array\n",
    "    filtered_points = np.asarray(filtered_points.points)\n",
    "    filtered_attributes = np.asarray(filtered_attributes)\n",
    "    # Extract X, Y coordinates and intensity from filtered points and attributes\n",
    "    X, Y = filtered_points[:, 0], filtered_points[:, 1]\n",
    "    intensity = filtered_attributes[:, 1]  # Assuming intensity is at index 1\n",
    "\n",
    "    # Define bins for X-coordinate\n",
    "    x_min, x_max = np.min(X), np.max(X)\n",
    "    bins = np.linspace(x_min, x_max, num_bins + 1)\n",
    "\n",
    "    # Bin points based on X-coordinate\n",
    "    bin_indices = np.digitize(X, bins) - 1  # Adjusting indices to be 0-based\n",
    "\n",
    "    # Analyze points in each bin\n",
    "    for bin_index in range(num_bins):\n",
    "        # Select points and attributes in current bin\n",
    "        in_bin_mask = bin_indices == bin_index\n",
    "        points_in_bin = filtered_points[in_bin_mask]\n",
    "        intensity_in_bin = intensity[in_bin_mask]\n",
    "        \n",
    "        lane_width = 3.9  # Assuming a constant lane width\n",
    "        # Skip empty bins\n",
    "        if points_in_bin.size == 0:\n",
    "            continue\n",
    "        \n",
    "        # in the y_peak_coordinates, if two peaks are at proximity of each other, then they are the same lane, combine them by calculating the average\n",
    "        y_peak_coordinates = np.sort(np.array(y_peak_coordinates))\n",
    "        \n",
    "        # merge_close_peaks\n",
    "        i = 0\n",
    "        # Iterate over the peaks; since the list might change size, use a while loop\n",
    "        while i < len(y_peak_coordinates) - 1:\n",
    "            # Check if the next peak is within lane_width/3 of the current peak\n",
    "            if y_peak_coordinates[i + 1] - y_peak_coordinates[i] <= lane_width / 3:\n",
    "                # Average the current peak and the next one\n",
    "                avg_peak = np.mean([y_peak_coordinates[i], y_peak_coordinates[i + 1]])\n",
    "                # Replace the two peaks with their average\n",
    "                y_peak_coordinates = np.delete(y_peak_coordinates, [i, i + 1])\n",
    "                y_peak_coordinates = np.insert(y_peak_coordinates, i, avg_peak)\n",
    "                # No need to increment i, as we want to check the next set of peaks against the newly formed average\n",
    "            else:\n",
    "                # Only increment if no merge was done, to move to the next peak\n",
    "                i += 1\n",
    "        \n",
    "        # Assign points to lanes based on Y-coordinates and intensity\n",
    "        for lane_index, y_peak in enumerate(y_peak_coordinates):\n",
    "            # Define lane boundaries based on peak Y-coordinate\n",
    "            y_min, y_max = y_peak - lane_width/2, y_peak + lane_width/2\n",
    "\n",
    "            # Identify points within the lane boundaries and above intensity threshold\n",
    "            lane_mask = (points_in_bin[:, 1] >= y_min) & (points_in_bin[:, 1] <= y_max) & (intensity_in_bin > threshold_intensity)\n",
    "            points_in_lane = points_in_bin[lane_mask]\n",
    "                \n",
    "            # Update lane groups if any points are identified\n",
    "            if points_in_lane.size > 0:\n",
    "                lane_groups[(bin_index, lane_index)] = points_in_lane\n",
    "\n",
    "    return lane_groups\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lane_groups_by_lane_number(pointcloud, lane_groups):\n",
    "    \"\"\"\n",
    "    Visualizes the lane groups by coloring the points in each lane with a consistent color across all X bins.\n",
    "    Points not belonging to any lane are colored white.\n",
    "    \n",
    "    Parameters:\n",
    "    - pointcloud: Open3D pointcloud object.\n",
    "    - lane_groups: A dictionary where keys are tuples (bin_index, lane_index) and values are numpy arrays of points assigned to a lane.\n",
    "    \"\"\"\n",
    "    # Initialize a color array for all points in the point cloud to white\n",
    "    colors = np.ones((np.asarray(pointcloud.points).shape[0], 3))  # Set all points to white initially\n",
    "\n",
    "    # Generate a color map with a unique color for each lane\n",
    "    num_lanes = max(lane_index for _, lane_index in lane_groups.keys()) + 1\n",
    "    unique_colors = plt.get_cmap(\"tab20\")(np.linspace(0, 1, num_lanes))\n",
    "\n",
    "    # Flatten the point cloud for easy indexing\n",
    "    flattened_points = np.asarray(pointcloud.points).reshape((-1, 3))\n",
    "\n",
    "    # Assign colors based on lane number\n",
    "    for (bin_index, lane_index), group_points in lane_groups.items():\n",
    "        color = unique_colors[lane_index][:3]  # Color based on lane_index only\n",
    "        for point in group_points:\n",
    "            idx = np.where((flattened_points == point).all(axis=1))[0]\n",
    "            if len(idx) > 0:\n",
    "                colors[idx[0]] = color  # Assign the specific color to matching points\n",
    "\n",
    "    # Update point cloud colors\n",
    "    pointcloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Visualize the point cloud with colored lane groups\n",
    "    o3d.visualization.draw_geometries([pointcloud], point_show_normal=False, window_name=\"Lane Groups by Lane Number Visualization - Modified\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lane_points(lane_groups):\n",
    "    \"\"\"\n",
    "    Extracts and aggregates points for each lane number from lane_groups.\n",
    "    \n",
    "    Parameters:\n",
    "    - lane_groups: A dictionary where keys are tuples (bin_index, lane_index) and values are numpy arrays of points assigned to a lane.\n",
    "    \n",
    "    Returns:\n",
    "    - lanes: A dictionary where keys are lane_index and values are aggregated numpy arrays of points for that lane.\n",
    "    \"\"\"\n",
    "    lanes = {}\n",
    "    for (bin_index, lane_index), points in lane_groups.items():\n",
    "        if lane_index not in lanes:\n",
    "            lanes[lane_index] = points\n",
    "        else:\n",
    "            # Concatenate points from the same lane across different bins\n",
    "            lanes[lane_index] = np.vstack((lanes[lane_index], points))\n",
    "            \n",
    "    return lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lane_polynomials(lane_groups, reference_y=0):\n",
    "    \"\"\"\n",
    "    Fits cubic polynomials to each lane and selects the lanes directly to the left and right of the reference point (y = 0).\n",
    "\n",
    "    Parameters:\n",
    "    - lane_groups: A dictionary containing lane points, where keys are (bin_index, lane_index) and values are numpy arrays of points.\n",
    "    - reference_y: The Y-coordinate of the reference point to determine the lanes to the left and right.\n",
    "\n",
    "    Returns:\n",
    "    - lane_fits: A dictionary with keys 'left' and 'right', containing the polynomial coefficients of the lanes directly to the left and right of the reference point.\n",
    "    \"\"\"\n",
    "    closest_left_lane = None\n",
    "    closest_right_lane = None\n",
    "    \n",
    "    min_left_distance = float('inf')\n",
    "    min_right_distance = float('inf')\n",
    "    \n",
    "    lanes = extract_lane_points(lane_groups)\n",
    "    \n",
    "    # find the left and right lanes based on the reference point\n",
    "    for lane_index, points in lanes.items():\n",
    "        avg_y = np.mean(points[:, 1]) # Average Y-coordinate of the lane\n",
    "        if avg_y < 0:  # Lane is to the left of y = 0\n",
    "            distance = abs(avg_y)\n",
    "            if distance < min_left_distance:\n",
    "                min_left_distance = distance\n",
    "                closest_left_lane = lane_index\n",
    "        elif avg_y > 0:  # Lane is to the right of y = 0\n",
    "            distance = abs(avg_y)\n",
    "            if distance < min_right_distance:\n",
    "                min_right_distance = distance\n",
    "                closest_right_lane = lane_index\n",
    "\n",
    "\n",
    "    # Function to fit a cubic polynomial and calculate least squares error\n",
    "    def fit_polynomial(points):\n",
    "        # Fit a cubic polynomial (degree=3)\n",
    "        x = points[:, 0]\n",
    "        y = points[:, 1]\n",
    "        coefs, residuals, _, _, _ = np.polyfit(x, y, 3, full=True)\n",
    "        return coefs, residuals\n",
    "\n",
    "    lane_fits = {'left': None, 'right': None}\n",
    "\n",
    "    # Fit the closest lane on each side, if any\n",
    "    if closest_left_lane:\n",
    "        # lanes[closest_left_lane]) contains x,y,z coordinates of the points in the lane, we only need x and y\n",
    "        coefs, residuals = fit_polynomial(lanes[closest_left_lane][:, :2])\n",
    "        lane_fits['left'] = {'coefs': coefs, 'residuals': residuals}\n",
    "\n",
    "    if closest_right_lane:\n",
    "        coefs, residuals = fit_polynomial(lanes[closest_right_lane][:, :2])\n",
    "        lane_fits['right'] = {'coefs': coefs, 'residuals': residuals}\n",
    "\n",
    "    return lane_fits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_grid_dict(min_x, max_x, max_lane_width=3.9, line_of_sight=10):\n",
    "    \"\"\"\n",
    "    Creates a grid dictionary for a given range of x-coordinates.\n",
    "\n",
    "    Args:\n",
    "    - min_x (int): The minimum x-coordinate.\n",
    "    - max_x (int): The maximum x-coordinate.\n",
    "    - max_lane_width (float, optional): The maximum width of a lane. Defaults to 3.9.\n",
    "    - line_of_sight (int, optional): The line of sight distance used to calculate grid bounds. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with grid coordinates as keys and grid bounds as values.\n",
    "    \"\"\"\n",
    "    grid_dict = {}\n",
    "    for y in (-1, 1):\n",
    "        for x in range(min_x, max_x + 1):\n",
    "            if y == -1:\n",
    "                grid_x_up_bound = x * line_of_sight\n",
    "                grid_x_low_bound = (x - 1) * line_of_sight\n",
    "                grid_y_up_bound = (y + 1) * max_lane_width\n",
    "                grid_y_low_bound = y * max_lane_width\n",
    "            else:\n",
    "                grid_x_up_bound = x * line_of_sight\n",
    "                grid_x_low_bound = (x - 1) * line_of_sight\n",
    "                grid_y_up_bound = y * max_lane_width\n",
    "                grid_y_low_bound = (y - 1) * max_lane_width\n",
    "\n",
    "            grid = [grid_x_up_bound, grid_x_low_bound, grid_y_up_bound, grid_y_low_bound]\n",
    "            coord = (y, x)\n",
    "            grid_dict[coord] = grid\n",
    "    return grid_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lidar_data_by_grid(lidar_data, grid_dict):\n",
    "    \"\"\"\n",
    "    Filters LiDAR data points based on the grid definitions, assigning points to the corresponding grid cells.\n",
    "\n",
    "    Args:\n",
    "    - lidar_data (numpy.ndarray): The LiDAR data points array.\n",
    "    - grid_dict (dict): The dictionary with grid coordinates as keys and grid bounds as values.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary mapping each grid cell to the LiDAR data points within that cell.\n",
    "    \"\"\"\n",
    "    data_in_grid = {}\n",
    "    for grid_cell_coord, bounds in grid_dict.items():\n",
    "        x_upper_bound, x_lower_bound, y_upper_bound, y_lower_bound = bounds\n",
    "        # Filter LiDAR data points within the current grid cell bounds\n",
    "        filtered_data = lidar_data[(lidar_data[:, 0] <= x_upper_bound) &\n",
    "                                   (lidar_data[:, 0] > x_lower_bound) &\n",
    "                                   (lidar_data[:, 1] <= y_upper_bound) &\n",
    "                                   (lidar_data[:, 1] > y_lower_bound)]\n",
    "        data_in_grid[grid_cell_coord] = filtered_data\n",
    "    return data_in_grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression(object):\n",
    "    def __init__(self, degree=3, coeffs=None):\n",
    "        self.degree = degree\n",
    "        self.coeffs = coeffs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.coeffs = np.polyfit(X.ravel(), y, self.degree)\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return {'coeffs': self.coeffs}\n",
    "\n",
    "    def set_params(self, coeffs=None, random_state=None):\n",
    "        self.coeffs = coeffs\n",
    "\n",
    "    def predict(self, X):\n",
    "        poly_eqn = np.poly1d(self.coeffs)\n",
    "        y_hat = poly_eqn(X.ravel())\n",
    "        return y_hat\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return mean_squared_error(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(up_lane_coeffs, down_lane_coeffs, x_range):\n",
    "    \"\"\"\n",
    "    Calculate the cost as the mean squared error between the true lane width (assumed constant)\n",
    "    and the measured lane width across a range of x-values.\n",
    "\n",
    "    Args:\n",
    "    - up_lane_coeffs (array): Coefficients for the polynomial representing the upper lane boundary.\n",
    "    - down_lane_coeffs (array): Coefficients for the polynomial representing the lower lane boundary.\n",
    "    - x_range (array): Range of x-values to evaluate the lane width.\n",
    "\n",
    "    Returns:\n",
    "    - float: The cost calculated as mean squared error.\n",
    "    \"\"\"\n",
    "    # Define polynomial functions for the upper and lower lanes\n",
    "    poly_up = np.poly1d(up_lane_coeffs)\n",
    "    poly_down = np.poly1d(down_lane_coeffs)\n",
    "\n",
    "    # Calculate derivatives for the lower lane polynomial\n",
    "    poly_down_deriv = np.polyder(poly_down)\n",
    "\n",
    "    # Vectorized calculation of y-values for lower and upper lanes\n",
    "    y_down = poly_down(x_range)\n",
    "    y_deriv_down = poly_down_deriv(x_range)\n",
    "\n",
    "    # Initialize measured intervals\n",
    "    interval_measured = []\n",
    "\n",
    "    # Calculate perpendicular distances for each x in x_range\n",
    "    for x, y, dy in zip(x_range, y_down, y_deriv_down):\n",
    "        # Define the perpendicular line at (x, y) of the lower lane\n",
    "        perp_slope = -1 / dy if dy != 0 else np.inf\n",
    "        y_intercept = y - perp_slope * x\n",
    "\n",
    "        # Define a function for the intersection with the upper lane\n",
    "        def intersection_fun(x_intersect):\n",
    "            return perp_slope * x_intersect + y_intercept - poly_up(x_intersect)\n",
    "\n",
    "        # Use fsolve to find the intersection point\n",
    "        x_intersect = fsolve(intersection_fun, x)[0]\n",
    "\n",
    "        # Calculate the perpendicular distance\n",
    "        y_intersect = perp_slope * x_intersect + y_intercept\n",
    "        dist = distance.euclidean([x_intersect, y_intersect], [x, y])\n",
    "        interval_measured.append(dist)\n",
    "\n",
    "    # True interval is assumed to be constant (e.g., 3 meters)\n",
    "    interval_truth = np.full_like(x_range, 3)\n",
    "\n",
    "    # Calculate MSE as the cost\n",
    "    cost = mean_squared_error(interval_truth, interval_measured)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'eps': 0.79, 'min_samples': 5}, Best score: -1.2459419576979833\n",
      "[Open3D DEBUG] Precompute neighbors.\n",
      "[Open3D DEBUG] Done Precompute neighbors.\n",
      "[Open3D DEBUG] Compute Clusters\n",
      "[Open3D DEBUG] Done Compute Clusters: 34\n",
      "Point cloud has 34 clusters\n",
      "Filtered 26215 points from 29621 to 3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:447: RankWarning: Polyfit may be poorly conditioned\n",
      "  estimator.fit(X_subset, y_subset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent: 40.30 sec\n",
      "0.790888441043202\n",
      "Best parameters: {'eps': 0.85, 'min_samples': 20}, Best score: -2.966763985962429\n",
      "[Open3D DEBUG] Precompute neighbors.\n",
      "[Open3D DEBUG] Done Precompute neighbors.\n",
      "[Open3D DEBUG] Compute Clusters\n",
      "[Open3D DEBUG] Done Compute Clusters: 44\n",
      "Point cloud has 44 clusters\n",
      "Filtered 25002 points from 28086 to 3084\n",
      "time spent: 52.17 sec\n",
      "0.6274885208869833\n",
      "Best parameters: {'eps': 0.88, 'min_samples': 10}, Best score: -3.7738126894644473\n",
      "[Open3D DEBUG] Precompute neighbors.\n",
      "[Open3D DEBUG] Done Precompute neighbors.\n",
      "[Open3D DEBUG] Compute Clusters\n",
      "[Open3D DEBUG] Done Compute Clusters: 43\n",
      "Point cloud has 43 clusters\n",
      "Filtered 27824 points from 30038 to 2214\n",
      "time spent: 36.36 sec\n",
      "0.13559856183231364\n",
      "Best parameters: {'eps': 0.88, 'min_samples': 20}, Best score: -24.179367678548935\n",
      "[Open3D DEBUG] Precompute neighbors.\n",
      "[Open3D DEBUG] Done Precompute neighbors.\n",
      "[Open3D DEBUG] Compute Clusters\n",
      "[Open3D DEBUG] Done Compute Clusters: 67\n",
      "Point cloud has 67 clusters\n",
      "Filtered 63822 points from 68671 to 4849\n",
      "time spent: 40.21 sec\n",
      "0.629328212214307\n",
      "Best parameters: {'eps': 0.88, 'min_samples': 20}, Best score: -51.37352975738994\n",
      "[Open3D DEBUG] Precompute neighbors.\n",
      "[Open3D DEBUG] Done Precompute neighbors.\n",
      "[Open3D DEBUG] Compute Clusters\n",
      "[Open3D DEBUG] Done Compute Clusters: 87\n",
      "Point cloud has 87 clusters\n",
      "Filtered 79382 points from 86003 to 6621\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`min_samples` may not be larger than number of samples: n_samples = 18.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 121\u001b[0m\n\u001b[1;32m    115\u001b[0m right_grids_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(data_repres_right, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    117\u001b[0m ransac_up \u001b[38;5;241m=\u001b[39m RANSACRegressor(PolynomialRegression(degree \u001b[38;5;241m=\u001b[39m poly_degree),\n\u001b[1;32m    118\u001b[0m                         min_samples \u001b[38;5;241m=\u001b[39m min_samples,    \n\u001b[1;32m    119\u001b[0m                         max_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m    120\u001b[0m                         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[43mransac_up\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_grids_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_grids_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m left_grids_y_pred \u001b[38;5;241m=\u001b[39m ransac_up\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(left_grids_x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    123\u001b[0m left_lane_coeffs\u001b[38;5;241m=\u001b[39m ransac_up\u001b[38;5;241m.\u001b[39mestimator_\u001b[38;5;241m.\u001b[39mget_params(deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoeffs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_ransac.py:358\u001b[0m, in \u001b[0;36mRANSACRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    356\u001b[0m     min_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_samples \u001b[38;5;241m>\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`min_samples` may not be larger than number \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof samples: n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# MAD (median absolute deviation)\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     residual_threshold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(np\u001b[38;5;241m.\u001b[39mabs(y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: `min_samples` may not be larger than number of samples: n_samples = 18."
     ]
    }
   ],
   "source": [
    "# call the function to load the point clouds\n",
    "folder_path = './pointclouds/'\n",
    "pointclouds_with_attributes = load_pointclouds_with_attributes(folder_path)\n",
    "\n",
    "# create a list of filename which with extensoin .bin\n",
    "file_name = [os.path.basename(file) for file in os.listdir(folder_path) if file.endswith('.bin')]\n",
    "\n",
    "for i, (pcd, attributes) in enumerate(pointclouds_with_attributes):\n",
    "    # filter the point cloud based on the z-axis\n",
    "    selected_indices = z_filter(pcd)\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.asarray(pcd.points)[selected_indices])\n",
    "    attributes = attributes[selected_indices]\n",
    "    \n",
    "    \n",
    "    #  explore the parameter space and visualize the results\n",
    "    eps_values = np.arange(0.01, 0.9, 0.03)  # Example range for eps\n",
    "    min_samples_values = range(5, 30, 5)  # Example range for min_samples\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    best_params = {'eps': None, 'min_samples': None}\n",
    "\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            num_clusters, noise_ratio = evaluate_clustering(pcd, eps, min_samples)\n",
    "            \n",
    "            # Calculate the clustering score\n",
    "            current_score = score_clustering(num_clusters, noise_ratio)\n",
    "            \n",
    "            # Update best parameters if current score is better\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_params['eps'] = eps\n",
    "                best_params['min_samples'] = min_samples\n",
    "\n",
    "    print(f\"Best parameters: {best_params}, Best score: {best_score}\")\n",
    "\n",
    "    eps = best_params['eps']  # Use tuned eps value\n",
    "    min_samples = best_params['min_samples']  # Use tuned min_samples value\n",
    "    \n",
    "    ground_pcd, pcd, ground_attributes, attributes= segment_ground_plane(pcd, attributes, distance_threshold=(eps*0.005), ransac_n=5, num_iterations=1000)\n",
    "    \n",
    "    # Perform DBSCAN and update attributes\n",
    "    updated_pcd, updated_attributes = perform_dbscan_and_update_attributes((pcd, attributes), eps, min_samples)\n",
    "    # Learn intensity threshold from clusters    \n",
    "    intensity_threshold = learn_intensity_threshold_from_clusters(updated_attributes, percentage=0.5)\n",
    "    \n",
    "    # Filter clusters based on intensity and geometric shape\n",
    "    selected_indices = filter_clusters_based_on_intensity(updated_attributes, intensity_threshold)\n",
    "    \n",
    "    # Extract filtered points for visualization or further processing\n",
    "    filtered_points = np.asarray(updated_pcd.points)[selected_indices]\n",
    "    \n",
    "    # calculate the delta of the points before and after filtering\n",
    "    delta = len(np.asarray(updated_pcd.points)) - len(filtered_points)\n",
    "    \n",
    "    print(f\"Filtered {delta} points from {len(np.asarray(updated_pcd.points))} to {len(filtered_points)}\")\n",
    "    \n",
    "    # Create a new point cloud object for filtered points, if needed\n",
    "    filtered_pcd = o3d.geometry.PointCloud()\n",
    "    filtered_pcd.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "    filtered_attributes = updated_attributes[selected_indices]\n",
    "\n",
    "    # visualize_lane_detection(pcd, filtered_pcd)\n",
    "\n",
    "    # Replace the original pcd and attributes with the filtered ones\n",
    "    pointclouds_with_attributes[i] = (filtered_pcd, filtered_attributes)\n",
    "    # num_lanes, y_peaks_coordinates, intensity_threshold = find_number_of_lanes(filtered_pcd, filtered_attributes, percentile = 5, min_num_peaks=2)\n",
    "    \n",
    "    # conver pointcloud to np.array\n",
    "    filtered_pcd_array = np.asarray(filtered_pcd.points)\n",
    "    min_x = np.floor(np.min(filtered_pcd_array[:, 0])).astype(int)\n",
    "    max_x = np.ceil(np.max(filtered_pcd_array[:, 0])).astype(int) \n",
    "    \n",
    "    grid_dict = create_grid_dict(min_x, max_x)\n",
    "    data_in_grid = filter_lidar_data_by_grid(filtered_pcd_array, grid_dict)\n",
    "    \n",
    "    poly_degree = 3\n",
    "    # shape of lidar_data\n",
    "    n = filtered_pcd_array.shape[1]\n",
    "    \n",
    "    data_repres_left = np.empty((0, n))\n",
    "    data_repres_right = np.empty((0, n)) \n",
    "\n",
    "    iteration = 0\n",
    "    max_iter = 1000\n",
    "    prev_error = 1000\n",
    "\n",
    "    start = time.time()\n",
    "    while iteration <= max_iter:\n",
    "        for y in (-1, 1):\n",
    "            for x in range(min_x, max_x + 1):\n",
    "                if y == -1:\n",
    "                    if len(data_in_grid[y, x]) >= min_samples:\n",
    "                        idx = np.random.randint(len(data_in_grid[y, x]), size = poly_degree)\n",
    "                        data_repres_left = np.append(data_repres_left, data_in_grid[y, x][idx, :], axis = 0)\n",
    "                    elif len(data_in_grid[y, x]) == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        idx = np.random.randint(len(data_in_grid[y, x]), size = len(data_in_grid[y,x]))\n",
    "                        data_repres_left = np.append(data_repres_left, data_in_grid[y, x][idx, :], axis = 0)\n",
    "\n",
    "                elif y == 1:\n",
    "                    if len(data_in_grid[y, x]) >= min_samples:\n",
    "                        idx = np.random.randint(len(data_in_grid[y, x]), size = poly_degree)\n",
    "                        data_repres_right = np.append(data_repres_right, data_in_grid[y, x][idx, :], axis = 0)\n",
    "                    elif len(data_in_grid[y, x]) == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        idx = np.random.randint(len(data_in_grid[y, x]), size = len(data_in_grid[y,x]))\n",
    "                        data_repres_right = np.append(data_repres_right, data_in_grid[y, x][idx, :], axis = 0)\n",
    "                        \n",
    "        left_grids_x = np.sort(data_repres_left, axis = 0)[:, 0] \n",
    "        left_grids_y = np.sort(data_repres_left, axis = 0)[:, 1]\n",
    "        right_grids_x = np.sort(data_repres_right, axis = 0)[:, 0] \n",
    "        right_grids_y = np.sort(data_repres_right, axis = 0)[:, 1]\n",
    "\n",
    "        ransac_up = RANSACRegressor(PolynomialRegression(degree = poly_degree),\n",
    "                                min_samples = int(min_samples*0.6),    \n",
    "                                max_trials = 10000,\n",
    "                                random_state=0)\n",
    "        ransac_up.fit(np.expand_dims(left_grids_x, axis=1), left_grids_y)\n",
    "        left_grids_y_pred = ransac_up.predict(np.expand_dims(left_grids_x, axis=1))\n",
    "        left_lane_coeffs= ransac_up.estimator_.get_params(deep = True)[\"coeffs\"]\n",
    "\n",
    "        ransac_down = RANSACRegressor(PolynomialRegression(degree = poly_degree), \n",
    "                                min_samples = int(min_samples*0.6),    \n",
    "                                max_trials = 10000,\n",
    "                                random_state=0)\n",
    "        ransac_down.fit(np.expand_dims(right_grids_x, axis=1), right_grids_y)\n",
    "        right_grids_y_pred = ransac_down.predict(np.expand_dims(right_grids_x, axis=1))\n",
    "        right_lane_coeffs = ransac_down.estimator_.get_params(deep = True)[\"coeffs\"]\n",
    "        \n",
    "        ego_lane_coeffs_pair = np.append(right_lane_coeffs, left_lane_coeffs, axis = 0) \n",
    "        curr_error = cost(left_lane_coeffs, right_lane_coeffs, np.linspace(min_x, max_x, num=100))      \n",
    "        \n",
    "        if curr_error < prev_error:\n",
    "            prev_error = curr_error \n",
    "            best_coeffs_pair = ego_lane_coeffs_pair\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    print(\"time spent: {:.2f} sec\".format(time.time() - start)) \n",
    "    print(prev_error)\n",
    "\n",
    "    # Convert the best_coeffs_pair to a 2D array with 4 columns\n",
    "    best_coeffs_pair = best_coeffs_pair.reshape(-1, 4)\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = 'sample_output'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Save the coefficients to a text file\n",
    "    lidar_txt_name = file_name[i].replace('.bin', '.txt')\n",
    "    # save both left and right lane coefficients as two rows in the text file\n",
    "    np.savetxt(os.path.join(output_dir, lidar_txt_name), best_coeffs_pair, delimiter=';', fmt='%.15e')        \n",
    "    \n",
    "    # fh = open(f'scene{i}', 'bw')\n",
    "    # # save the point cloud to file as float32\n",
    "    # np.asarray(filtered_pcd.points).astype('float32').tofile(fh)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
